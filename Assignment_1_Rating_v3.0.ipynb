{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258 Assignment 1 Rating\n",
    "**Ming Ki Toby Cheng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, \"rt\"):\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, \"rt\")\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining MSE function\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    if user not in userBiases and item in itemBiases:\n",
    "        return alpha + itemBiases[item]\n",
    "    if user in userBiases and item not in itemBiases:\n",
    "        return alpha + userBiases[user]\n",
    "    if user not in userBiases and item not in itemBiases:\n",
    "        return alpha\n",
    "    if user in userBiases and item in itemBiases:\n",
    "        return alpha + userBiases[user] + itemBiases[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "books = []\n",
    "ratings = []\n",
    "\n",
    "for user, book, _ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    users.append(user)\n",
    "    books.append(book)\n",
    "    ratings.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initializing data\n",
    "users_train = users[:190000]\n",
    "books_train = books[:190000]\n",
    "ratings_train = ratings[:190000]\n",
    "users_valid = users[190000:]\n",
    "books_valid = books[190000:]\n",
    "ratings_valid = ratings[190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rating = list(zip(users_train, books_train, ratings_train))\n",
    "validation_rating = list(zip(users_valid, books_valid, ratings_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating = []\n",
    "userRatings = defaultdict(list)\n",
    "bookRatings = defaultdict(list)\n",
    "\n",
    "\n",
    "for user, book, r in training_rating:\n",
    "    train_rating.append(int(r))\n",
    "    userRatings[user].append(int(r))\n",
    "    bookRatings[book].append(int(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAverage = sum(train_rating) / len(train_rating)\n",
    "\n",
    "alpha = trainAverage\n",
    "nUsers = len(userRatings)\n",
    "nItems = len(bookRatings)\n",
    "users = list(userRatings.keys())\n",
    "items = list(bookRatings.keys())\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d[0], d[1]) for d in training_rating]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(training_rating)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in training_rating:\n",
    "        u,i = d[0], d[1]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - int(d[2])\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(d[2]) for d in training_rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4806916922249362\n",
      "MSE = 2.300129305099145\n",
      "MSE = 1.4733843875018926\n",
      "MSE = 1.4732252256322795\n",
      "MSE = 1.472589045174189\n",
      "MSE = 1.4700517956676158\n",
      "MSE = 1.4600223548536508\n",
      "MSE = 1.4218175069968622\n",
      "MSE = 1.070216311137085\n",
      "MSE = 1.1392270770979693\n",
      "MSE = 1.015447885961261\n",
      "MSE = 0.9931982668371578\n",
      "MSE = 0.9543919633735973\n",
      "MSE = 0.942082305551534\n",
      "MSE = 0.9362790208580444\n",
      "MSE = 0.9324379184063721\n",
      "MSE = 0.9283485644493698\n",
      "MSE = 0.9237191530780602\n",
      "MSE = 0.9223042482718358\n",
      "MSE = 0.9224755182136947\n",
      "MSE = 0.9208647934180683\n",
      "MSE = 1.0443647242062977\n",
      "MSE = 0.9208597995802826\n",
      "MSE = 0.9208640557390156\n",
      "MSE = 0.9209496756164541\n",
      "MSE = 0.9208986314865779\n",
      "MSE = 0.9207212412243876\n",
      "MSE = 0.9203456334394361\n",
      "MSE = 0.920493383192212\n",
      "MSE = 0.9203020576035599\n",
      "MSE = 0.9202313593298367\n",
      "MSE = 0.9201388579393541\n",
      "MSE = 0.9200958606191081\n",
      "MSE = 0.920077018929819\n",
      "MSE = 0.9200851697200259\n",
      "MSE = 0.9200782918069236\n",
      "MSE = 0.9200795786163819\n",
      "MSE = 0.9200728073906611\n",
      "MSE = 0.920069884545226\n",
      "MSE = 0.9200687266953449\n",
      "MSE = 0.920070766206048\n",
      "MSE = 0.9200818562558392\n",
      "MSE = 0.9200872919503721\n",
      "MSE = 0.9200839990100379\n",
      "MSE = 0.9200865097006962\n",
      "MSE = 0.9200997422053621\n",
      "MSE = 0.9200992952036832\n",
      "MSE = 0.9200998086553099\n",
      "MSE = 0.9201017026973528\n",
      "MSE = 0.920102799126186\n",
      "MSE = 0.9201083906088895\n",
      "MSE = 0.9201124560460222\n",
      "MSE = 0.9201037125639694\n",
      "MSE = 0.9201118944892744\n",
      "MSE = 0.920116120077407\n",
      "MSE = 0.9201173445315702\n",
      "MSE = 0.9201177420121784\n",
      "MSE = 0.9201185640346243\n",
      "MSE = 0.9201198633223447\n",
      "MSE = 0.9201218785372967\n",
      "MSE = 0.920126370500358\n",
      "MSE = 0.9201234382111135\n",
      "MSE = 0.9201228202691593\n",
      "MSE = 0.9201369017183436\n",
      "MSE = 0.920125234060673\n",
      "MSE = 0.9201239134531597\n",
      "MSE = 0.9201230449867842\n",
      "MSE = 0.9201225802005952\n",
      "MSE = 0.9201216635746883\n",
      "MSE = 0.9201206303022553\n",
      "MSE = 0.9201199731690498\n",
      "MSE = 0.9201194261197397\n",
      "MSE = 0.9201198965890066\n",
      "MSE = 0.9201206238072573\n",
      "MSE = 0.9201216857064501\n",
      "MSE = 0.9201229089923912\n",
      "MSE = 0.92012354734751\n",
      "MSE = 0.9200884849323511\n",
      "MSE = 0.9201232305146664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.81259351, -0.00758701,  0.24722628, ..., -0.01909725,\n",
       "        -0.29646326, -0.6235612 ]),\n",
       " 0.9930888871038479,\n",
       " {'grad': array([-1.27239206e-04, -4.89260821e-08, -4.57111230e-08, ...,\n",
       "          1.52263815e-07,  2.28627730e-07,  2.67641548e-07]),\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 79,\n",
       "  'nit': 62,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 0.0000168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = []\n",
    "real_rate = []\n",
    "for user, book, rating in validation_rating:\n",
    "    val_pred.append(prediction(user,book))\n",
    "    real_rate.append(int(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1080579217390374"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(val_pred,real_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0000175        0.0000169          0.0000168          0.0000167\n",
    "1.10807495437515 1.1080549096625298 1.1080412812036005 1.1080477739367218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.000014           0.000016          0.000017           0.000018\n",
    "1.1083714213430038 1.108064308947268 1.1080559417223608 1.108135090292363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.000009          0.0000095          0.000015           0.00002           0.000012\n",
    "1.111127818180121 1.1106934089828682 1.1081628940572545 1.108512995969658 1.1090584867923212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda:0.0001,  0.00001             0.000005            0.000008          0.0000095\n",
    "MSE: 1.17724    1.1103061733208988  1.1163570457783654  1.1121307718734341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda: 0.001, 1, 3\n",
    "MSE: 1.39810, 1.4907, 1.4908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing predictions of test set to file\n",
    "predictions = open(\"predictions_Rating_Assignment1_v3.0.txt\", \"w\")\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    \n",
    "    predictions.write(u + \"-\" + b + \",\" + str(prediction(u,b)) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using previous lambda now use all the data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratngs = list(zip(users, books, ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming dictionary with user and book combinations\n",
    "ratingPerCombo = {}\n",
    "usersPerBook = defaultdict(set)\n",
    "booksPerUser = defaultdict(set)\n",
    "\n",
    "for user, book,r in all_ratings:\n",
    "    usersPerBook[book].add(user)\n",
    "    booksPerUser[user].add(book)\n",
    "    ratingPerCombo[(user,book)] = int(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all user ratings and all books dictionaries\n",
    "totalRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "bookRatings = defaultdict(list)\n",
    "\n",
    "# All ratings for each user and each book\n",
    "for user, book, r in all_ratings:\n",
    "    r = int(r)\n",
    "    totalRatings.append(r)\n",
    "    userRatings[user].append(r)\n",
    "    bookRatings[book].append(r)\n",
    "\n",
    "globalAverage = sum(totalRatings) / len(totalRatings)\n",
    "userAverage = {}\n",
    "userBias = {}\n",
    "user_total = 0\n",
    "bookAverage = {}\n",
    "bookBias = {}\n",
    "book_total = 0\n",
    "\n",
    "# Initializing user and book biases\n",
    "for user in userRatings:\n",
    "    userAverage[user] = sum(userRatings[user]) / len(userRatings[user])\n",
    "    \n",
    "for user in userAverage:\n",
    "    user_total += float(userAverage[user])\n",
    "    \n",
    "for user in userAverage:\n",
    "    userBias[user] = userAverage[user] - (user_total/len(userAverage))\n",
    "\n",
    "for book in bookRatings:\n",
    "    bookAverage[book] = sum(bookRatings[book]) / len(bookRatings[book])\n",
    "    \n",
    "for book in bookAverage:\n",
    "    book_total += float(bookAverage[book])\n",
    "    \n",
    "for book in bookAverage:\n",
    "    bookBias[book] = bookAverage[book] - (book_total/len(bookAverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining alpha, bookBias, userBias by convergence from Training data and lambda value 1\n",
    "lamb = 3.15\n",
    "alpha_sum = 0\n",
    "alpha = 0\n",
    "MSE_diff = 5\n",
    "trial = 0\n",
    "\n",
    "while MSE_diff > 0.00005 or trial > 1000:\n",
    "    model_predictions = []\n",
    "    actual_rating = []\n",
    "    alpha_sum = 0\n",
    "    \n",
    "    for user, book, r in all_ratings:\n",
    "        alpha_sum += (r-(userBias[user] + bookBias[book]))\n",
    "    alpha = alpha_sum/len(all_ratings)\n",
    "    \n",
    "    for user in booksPerUser:\n",
    "        beta_U_Sum = 0\n",
    "        for books in booksPerUser[user]:\n",
    "            beta_U_Sum += ratingPerCombo[user,books] - (alpha + bookBias[books])\n",
    "        userBias[user] = beta_U_Sum/(lamb+ len(booksPerUser[user]))\n",
    "    \n",
    "    for book in usersPerBook:\n",
    "        beta_I_Sum = 0\n",
    "        for users in usersPerBook[book]:\n",
    "            beta_I_Sum += ratingPerCombo[users,book] - (alpha + userBias[users])\n",
    "        bookBias[book] = beta_I_Sum/(lamb+ len(usersPerBook[book]))\n",
    "    \n",
    "    for user, book, r in all_ratings:\n",
    "        model_predictions.append(int(prediction(user, book)))\n",
    "        actual_rating.append(r)\n",
    "    \n",
    "    if trial == 0:\n",
    "        MSE_0 = 0\n",
    "\n",
    "    MSE_1 = MSE(model_predictions, actual_rating)\n",
    "    \n",
    "    MSE_diff = abs(MSE_1 - MSE_0)\n",
    "\n",
    "    print('MSE_new:', MSE_1)\n",
    "    \n",
    "    MSE_0 = MSE_1\n",
    "    trial += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and MSE on validation data\n",
    "model_predictions = []\n",
    "for user, book, r in validation_rating:\n",
    "    model_predictions.append(prediction(user, book))\n",
    "\n",
    "MSE_valid = MSE(model_predictions, ratings_valid)\n",
    "print('MSE on validation set:',MSE_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing predictions of test set to file\n",
    "predictions = open(\"predictions_Rating_Assignment1.txt\", \"w\")\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    \n",
    "    predictions.write(u + \"-\" + b + \",\" + str(prediction(u,b)) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Username: tobycheng or Toby Cheng**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Rating MSE: 1.13707**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
