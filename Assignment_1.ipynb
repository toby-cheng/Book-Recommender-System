{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258 Assignment 1\n",
    "**Ming Ki Toby Cheng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, \"rt\"):\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, \"rt\")\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "books = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, book, _ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    users.append(user)\n",
    "    books.append(book)\n",
    "    ratings.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and validation\n",
    "users_train = users[:190000]\n",
    "books_train = books[:190000]\n",
    "ratings_train = ratings[:190000]\n",
    "users_valid = users[190000:]\n",
    "books_valid = books[190000:]\n",
    "ratings_valid = ratings[190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCount = defaultdict(set)\n",
    "allBooks = set()\n",
    "\n",
    "for user, book, _ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    userCount[user].add(book)\n",
    "    allBooks.add(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_valid_new = users_valid[:]\n",
    "books_valid_new = books_valid[:]\n",
    "read_valid = [1] * len(users_valid)\n",
    "read_valid_new = read_valid[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating negative entries for validations randomly\n",
    "random.seed(1583)\n",
    "\n",
    "for users in users_valid:\n",
    "    unread_books = allBooks.difference(userCount[users])\n",
    "    unread_books_list = list(unread_books)\n",
    "    A = unread_books_list[random.randint(0, len(unread_books_list) - 1)]\n",
    "    users_valid_new.append(users)\n",
    "    books_valid_new.append(A)\n",
    "    read_valid_new.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_train_new = users_train[:]\n",
    "books_train_new = books_train[:]\n",
    "read_train = [1] * len(users_train)\n",
    "read_train_new = read_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1342)\n",
    "\n",
    "for users in users_train:\n",
    "    unread_books = allBooks.difference(userCount[users])\n",
    "    unread_books_list = list(unread_books)\n",
    "    A = unread_books_list[random.randint(0, len(unread_books_list) - 1)]\n",
    "    users_train_new.append(users)\n",
    "    books_train_new.append(A)\n",
    "    read_train_new.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = list(zip(users_valid_new, books_valid_new, read_valid_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = list(zip(users_train, books_train, read_train))\n",
    "training_set_neg = list(zip(users_train_new, books_train_new, read_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user, book,_ in training_set:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling validation data\n",
    "random.seed(1234)\n",
    "random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sets for users and books\n",
    "usersPerBook = defaultdict(set)\n",
    "booksPerUser = defaultdict(set)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "for user, book, rating in training_set:\n",
    "    usersPerBook[book].add(user)\n",
    "    booksPerUser[user].add(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Jaccard similarity function\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return (numer / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining separate Jaccard Prediction function\n",
    "def predictionJaccardReg_2(u, b):\n",
    "    similarities = []\n",
    "    books = booksPerUser[u]\n",
    "    users = usersPerBook[b]\n",
    "    for user in users:\n",
    "        books_other = booksPerUser[user]\n",
    "        sim = Jaccard(books, books_other)\n",
    "        similarities.append((sim, user))\n",
    "    similarities.sort(reverse = True)\n",
    "\n",
    "    if similarities[0][0] == 1.0 and len(similarities) > 1:\n",
    "        return(similarities[1][0])\n",
    "    elif len(similarities) == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(similarities[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u40837736', 'b16562997', 0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_neg[379999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionJaccardReg_2('u40837736', 'b16562997')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining separate Jaccard Prediction function\n",
    "def predictionJaccardReg(u, b):\n",
    "    similarities = []\n",
    "    books = booksPerUser[u]\n",
    "    users = usersPerBook[b]\n",
    "    for book in books:\n",
    "        users_other = usersPerBook[book]\n",
    "        sim = Jaccard(users, users_other)\n",
    "        similarities.append((sim, book))\n",
    "    similarities.sort(reverse = True)\n",
    "\n",
    "    if similarities[0][0] == 1.0 and len(similarities) > 1:\n",
    "        return(similarities[1][0])\n",
    "    else:\n",
    "        return(similarities[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Cosine similarity function\n",
    "def Cosine(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1) * len(s2)\n",
    "    if denom == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(numer / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining separate Cosine Prediction function\n",
    "def predictionCosineReg(u, b):\n",
    "    cos_sim = []\n",
    "    books = booksPerUser[u]\n",
    "    users = usersPerBook[b]\n",
    "    for book in books:\n",
    "        users_other = usersPerBook[book]\n",
    "        csim = Cosine(users, users_other)\n",
    "        cos_sim.append((csim, book))\n",
    "        \n",
    "    cos_sim.sort(reverse = True)\n",
    "\n",
    "    return(cos_sim[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining book read count for popularity\n",
    "bookPop = defaultdict(int)\n",
    "\n",
    "for bPop, b in mostPopular:\n",
    "    bookPop[b] = bPop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e53a800d44af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpopular_num_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbookPop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcosine_sim_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionCosineReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mjaccard_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionJaccardReg_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#if book in return1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#    predictions_reg.append(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-c86b9781957d>\u001b[0m in \u001b[0;36mpredictionJaccardReg_2\u001b[0;34m(u, b)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "jaccard_sim_max = []\n",
    "popular_num_train = []\n",
    "read_train_reg = []\n",
    "cosine_sim_max = []\n",
    "jaccard_user = []\n",
    "for user, book, read in training_set_neg:\n",
    "    jaccard_sim_max.append(predictionJaccardReg(user,book))\n",
    "    read_train_reg.append(read)\n",
    "    popular_num_train.append(bookPop[book])\n",
    "    cosine_sim_max.append(predictionCosineReg(user,book))\n",
    "    jaccard_user.append(predictionJaccardReg_2(user,book))\n",
    "    #if book in return1:\n",
    "    #    predictions_reg.append(1)\n",
    "    #else:\n",
    "    #    predictions_reg.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_sim_val = []\n",
    "popular_num_val = []\n",
    "read_val_reg = []\n",
    "cosine_val = []\n",
    "jaccard_user_val = []\n",
    "\n",
    "for user, book, _ in validation_set:\n",
    "    jaccard_sim_val.append(predictionJaccardReg(user, book))\n",
    "    read_val_reg.append(_)\n",
    "    popular_num_val.append(bookPop[book])\n",
    "    cosine_val.append(predictionCosineReg(user,book))\n",
    "    jaccard_user_val.append(predictionJaccardReg_2(user,book))\n",
    "\n",
    "    #if book in return1:\n",
    "    #    predictions_val.append(1)\n",
    "    #else:\n",
    "    #    predictions_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_reg = list(zip(popular_num_train, jaccard_sim_max, cosine_sim_max, read_train_reg, jaccard_user))\n",
    "validation_set_reg = list(zip(popular_num_val, jaccard_sim_val, cosine_val, read_val_reg, jaccard_user_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling training data\n",
    "random.seed(1234)\n",
    "random.shuffle(training_set_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, int(datum[0]), float(datum[1]), float(datum[2])] \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in training_set_reg]\n",
    "Y_train = [d[2] for d in training_set_reg ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(C=0.01, class_weight = 'balanced')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = [feature(d) for d in validation_set_reg]\n",
    "y_valid = [d[3] for d in validation_set_reg ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_train)\n",
    "predictions = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = model.coef_\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in training_set_reg]\n",
    "Y_train = [d[3] for d in training_set_reg ]\n",
    "\n",
    "X_valid = [feature(d) for d in validation_set_reg]\n",
    "y_valid = [d[3] for d in validation_set_reg ]\n",
    "\n",
    "regularization_value = []\n",
    "train_acc = []\n",
    "train_BER = []\n",
    "val_acc = []\n",
    "val_BER = []\n",
    "#numpy.arange(0.007, 0.009, 0.0001)\n",
    "\n",
    "for i in numpy.arange(0.00001, 0.00003, 0.000001):\n",
    "    model = linear_model.LogisticRegression(C=i)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    prediction = model.predict(X_train)\n",
    "    predictions = model.predict(X_valid)\n",
    "\n",
    "    TP_train = sum([(p and l) for (p, l) in zip(prediction, Y_train)])\n",
    "    FP_train = sum([(p and not l) for (p, l) in zip(prediction, Y_train)])\n",
    "    TN_train = sum([(not p and not l) for (p, l) in zip(prediction, Y_train)])\n",
    "    FN_train = sum([(not p and l) for (p, l) in zip(prediction, Y_train)])\n",
    "    accu_train = (TP_train + TN_train) / (TP_train + FP_train + TN_train + FN_train)\n",
    "\n",
    "    TPR_train = TP_train / (TP_train + FN_train)\n",
    "    TNR_train = TN_train / (TN_train + FP_train)\n",
    "    BER_train = 1 - 0.5 * (TPR_train + TNR_train)\n",
    "\n",
    "    TP_valid = sum([(p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "    FP_valid = sum([(p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "    TN_valid = sum([(not p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "    FN_valid = sum([(not p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "    accu_valid = (TP_valid + TN_valid) / (TP_valid + FP_valid + TN_valid + FN_valid)\n",
    "\n",
    "    TPR_valid = TP_valid / (TP_valid + FN_valid)\n",
    "    TNR_valid = TN_valid / (TN_valid + FP_valid)\n",
    "    BER_valid = 1 - 0.5 * (TPR_valid + TNR_valid)\n",
    "\n",
    "    regularization_value.append(i)\n",
    "    train_acc.append(accu_train)\n",
    "    train_BER.append(BER_train)\n",
    "    val_acc.append(accu_valid)\n",
    "    val_BER.append(BER_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, train_acc, 'r--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, val_acc, 'b--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, val_BER, 'b--', regularization_value, train_BER, 'r--')\n",
    "plt.ylabel('BER')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_acc.index(max(val_acc)), regularization_value[val_acc.index(max(val_acc))],  max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_BER.index(min(val_BER)), regularization_value[val_BER.index(min(val_BER))],  min(val_BER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in training_set_reg]\n",
    "Y_train = [d[2] for d in training_set_reg ]\n",
    "\n",
    "X_valid = [feature(d) for d in validation_set_reg]\n",
    "y_valid = [d[2] for d in validation_set_reg ]\n",
    "\n",
    "regularization_value = []\n",
    "train_acc = []\n",
    "train_BER = []\n",
    "val_acc = []\n",
    "val_BER = []\n",
    "\n",
    "for i in numpy.arange(0.007, 0.009, 0.0001):\n",
    "    model = linear_model.LogisticRegression(C=i)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    prediction = model.predict(X_train)\n",
    "    predictions = model.predict(X_valid)\n",
    "\n",
    "    TP_train = sum([(p and l) for (p, l) in zip(prediction, Y_train)])\n",
    "    FP_train = sum([(p and not l) for (p, l) in zip(prediction, Y_train)])\n",
    "    TN_train = sum([(not p and not l) for (p, l) in zip(prediction, Y_train)])\n",
    "    FN_train = sum([(not p and l) for (p, l) in zip(prediction, Y_train)])\n",
    "    accu_train = (TP_train + TN_train) / (TP_train + FP_train + TN_train + FN_train)\n",
    "\n",
    "    TPR_train = TP_train / (TP_train + FN_train)\n",
    "    TNR_train = TN_train / (TN_train + FP_train)\n",
    "    BER_train = 1 - 0.5 * (TPR_train + TNR_train)\n",
    "\n",
    "    TP_valid = sum([(p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "    FP_valid = sum([(p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "    TN_valid = sum([(not p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "    FN_valid = sum([(not p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "    accu_valid = (TP_valid + TN_valid) / (TP_valid + FP_valid + TN_valid + FN_valid)\n",
    "\n",
    "    TPR_valid = TP_valid / (TP_valid + FN_valid)\n",
    "    TNR_valid = TN_valid / (TN_valid + FP_valid)\n",
    "    BER_valid = 1 - 0.5 * (TPR_valid + TNR_valid)\n",
    "\n",
    "    regularization_value.append(i)\n",
    "    train_acc.append(accu_train)\n",
    "    train_BER.append(BER_train)\n",
    "    val_acc.append(accu_valid)\n",
    "    val_BER.append(BER_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, train_acc, 'r--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, val_acc, 'b--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_acc.index(max(val_acc)), regularization_value[val_acc.index(max(val_acc))],  max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, train_BER, 'r--')\n",
    "plt.ylabel('BER')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regularization_value, val_BER, 'b--')\n",
    "plt.ylabel('BER')\n",
    "plt.xlabel('Regularization Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_BER.index(min(val_BER)), regularization_value[val_BER.index(min(val_BER))],  min(val_BER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(C=0.000023)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCosineReg(user,book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing predictions of test set to file\n",
    "X_test = []\n",
    "count = 0\n",
    "\n",
    "predictions = open(\"predictions_Read_Assignment1.txt\", \"w\")\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    jaccard_test=predictionJaccardReg(u, b)\n",
    "    popular_test=bookPop[b]\n",
    "    cosine_test =predictionCosineReg(u,b)\n",
    "    X_test.append([1, popular_test, jaccard_test, cosine_test])\n",
    "\n",
    "prediction_test = model.predict(X_test)\n",
    "prediction_test = list(prediction_test)\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    predictions.write(u + \"-\" + b + ','+str(prediction_test[count])+'\\n')\n",
    "    count+=1\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing predictions of test set to file\n",
    "jaccard_test = []\n",
    "popular_test = []\n",
    "X_test = []\n",
    "count = 0\n",
    "\n",
    "predictions = open(\"predictions_Read_Assignment1.txt\", \"w\")\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    jaccard_test=predictionJaccardReg(u, b)\n",
    "    popular_test=bookPop[b]\n",
    "    X_test.append([1, popular_test, jaccard_test])\n",
    "\n",
    "prediction_test = model.predict(X_test)\n",
    "prediction_test = list(prediction_test)\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    predictions.write(u + \"-\" + b + ','+str(prediction_test[count])+'\\n')\n",
    "    count+=1\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions, accuracy and BER based on combination of new popularity and Jaccard functions\n",
    "predictions = []\n",
    "y_valid = []\n",
    "\n",
    "for users, books, read in validation_set:\n",
    "    y_valid.append(read)\n",
    "    if books in return1 and predictionJaccard(users, books) == 1:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "TP_valid = sum([(p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "FP_valid = sum([(p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "TN_valid = sum([(not p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "FN_valid = sum([(not p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "accu_valid = (TP_valid + TN_valid) / (TP_valid + FP_valid + TN_valid + FN_valid)\n",
    "\n",
    "TPR_valid = TP_valid / (TP_valid + FN_valid)\n",
    "TNR_valid = TN_valid / (TN_valid + FP_valid)\n",
    "BER_valid = 1 - 0.5 * (TPR_valid + TNR_valid)\n",
    "\n",
    "## Accuracy and BER of Model on Set\n",
    "print(\"Accuracy on validation set:\", accu_valid)\n",
    "print(\"BER on validation set:\", BER_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions, accuracy and BER based on combination of new popularity and Jaccard functions\n",
    "predictions = []\n",
    "y_valid = []\n",
    "Jacthres = []\n",
    "weightthres = []\n",
    "accur = []\n",
    "BER = []\n",
    "\n",
    "for num in numpy.arange(1, 3, 0.1):\n",
    "    for jacnum in numpy.arange(0.001,0.01,0.001):\n",
    "        predictions = []\n",
    "        y_valid = []\n",
    "        \n",
    "        def predictionJaccard2(u, b):\n",
    "            similarities = []\n",
    "            books = booksPerUser[u]\n",
    "            users = usersPerBook[b]\n",
    "            for book in books:\n",
    "                users_other = usersPerBook[book]\n",
    "                sim = Jaccard(users, users_other)\n",
    "                similarities.append((sim, book))\n",
    "            if max(similarities)[0] > jacnum:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        return1 = set()\n",
    "        count = 0\n",
    "        for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            return1.add(i)\n",
    "            if count > totalRead / num:\n",
    "                break\n",
    "\n",
    "        for users, books, read in validation_set:\n",
    "            y_valid.append(read)\n",
    "            if books in return1 or predictionJaccard2(users, books) == 1:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "        TP_valid = sum([(p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "        FP_valid = sum([(p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "        TN_valid = sum([(not p and not l) for (p, l) in zip(predictions, y_valid)])\n",
    "        FN_valid = sum([(not p and l) for (p, l) in zip(predictions, y_valid)])\n",
    "        accu_valid = (TP_valid + TN_valid) / (TP_valid + FP_valid + TN_valid + FN_valid)\n",
    "\n",
    "        TPR_valid = TP_valid / (TP_valid + FN_valid)\n",
    "        TNR_valid = TN_valid / (TN_valid + FP_valid)\n",
    "        BER_valid = 1 - 0.5 * (TPR_valid + TNR_valid)\n",
    "        \n",
    "        Jacthres.append(jacnum)\n",
    "        weightthres.append(num)\n",
    "        accur.append(accu_valid)\n",
    "        BER.append(BER_valid)\n",
    "        ## Accuracy and BER of Model on Set\n",
    "        #print('Jaccard threshold', jacnum)\n",
    "        #print('Most popular weight', num)\n",
    "        #print(\"Accuracy on validation set:\", accu_valid)\n",
    "        #print(\"BER on validation set:\", BER_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Jacthres[179])\n",
    "print(weightthres[179])\n",
    "print(max(accur))\n",
    "print(accur.index(max(accur)))\n",
    "print(min(BER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books in return1 and predictionJaccard(users, books) == 1:\n",
    "max(similarities)[0] > 0.008:\n",
    "totalRead / 1.6\n",
    "Accuracy on validation set: 0.66125\n",
    "BER on validation set: 0.33875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Username: tobycheng or Toby Cheng**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "books = []\n",
    "ratings = []\n",
    "for user, book, _ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    users.append(user)\n",
    "    books.append(book)\n",
    "    ratings.append(int(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initializing data\n",
    "users_train = users[:190000]\n",
    "books_train = books[:190000]\n",
    "ratings_train = ratings[:190000]\n",
    "users_valid = users[190000:]\n",
    "books_valid = books[190000:]\n",
    "ratings_valid = ratings[190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rating = list(zip(users_train, books_train, ratings_train))\n",
    "validation_rating = list(zip(users_valid, books_valid, ratings_valid))\n",
    "total_set = list(zip(users, books, ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming dictionary with user and book combinations\n",
    "ratingPerCombo = {}\n",
    "for user, book,r in total_set:\n",
    "    usersPerBook[book].add(user)\n",
    "    booksPerUser[user].add(book)\n",
    "    ratingPerCombo[(user,book)] = int(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all user ratings and all books dictionaries\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "bookRatings = defaultdict(list)\n",
    "\n",
    "# All ratings for each user and each book\n",
    "for user, book, r in total_set:\n",
    "    r = int(r)\n",
    "    allRatings.append(r)\n",
    "    userRatings[user].append(r)\n",
    "    bookRatings[book].append(r)\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "userAverage = {}\n",
    "userBias = {}\n",
    "user_total = 0\n",
    "bookAverage = {}\n",
    "bookBias = {}\n",
    "book_total = 0\n",
    "\n",
    "# Initializing user and book biases\n",
    "for user in userRatings:\n",
    "    userAverage[user] = sum(userRatings[user]) / len(userRatings[user])\n",
    "    \n",
    "for user in userAverage:\n",
    "    user_total += float(userAverage[user])\n",
    "    \n",
    "for user in userAverage:\n",
    "    userBias[user] = userAverage[user] - (user_total/len(userAverage))\n",
    "\n",
    "for book in bookRatings:\n",
    "    bookAverage[book] = sum(bookRatings[book]) / len(bookRatings[book])\n",
    "    \n",
    "for book in bookAverage:\n",
    "    book_total += float(bookAverage[book])\n",
    "    \n",
    "for book in bookAverage:\n",
    "    bookBias[book] = bookAverage[book] - (book_total/len(bookAverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining predictor function\n",
    "def prediction(user, book):\n",
    "    if user not in userBias and book in bookBias:\n",
    "        return alpha + bookBias[book]\n",
    "    if user in userBias and book not in bookBias:\n",
    "        return alpha + userBias[user]\n",
    "    if user not in userBias and book not in bookBias:\n",
    "        return alpha\n",
    "    if user in userBias and book in bookBias:\n",
    "        return alpha + userBias[user] + bookBias[book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining MSE function\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining alpha, bookBias, userBias by convergence from Training data and lambda value 1\n",
    "lamb = 1\n",
    "alpha_sum = 0\n",
    "alpha = 0\n",
    "MSE_diff = 5\n",
    "trial = 0\n",
    "\n",
    "while MSE_diff > 0.00005 or trial > 1000:\n",
    "    model_predictions = []\n",
    "    alpha_sum = 0\n",
    "    \n",
    "    for user, book, r in training_rating:\n",
    "        alpha_sum += (r-(userBias[user] + bookBias[book]))\n",
    "    alpha = alpha_sum/len(training_rating)\n",
    "    \n",
    "    for user in booksPerUser:\n",
    "        beta_U_Sum = 0\n",
    "        for books in booksPerUser[user]:\n",
    "            beta_U_Sum += ratingPerCombo[user,books] - (alpha + bookBias[books])\n",
    "        userBias[user] = beta_U_Sum/(lamb+ len(booksPerUser[user]))\n",
    "    \n",
    "    for book in usersPerBook:\n",
    "        beta_I_Sum = 0\n",
    "        for users in usersPerBook[book]:\n",
    "            beta_I_Sum += ratingPerCombo[users,book] - (alpha + userBias[users])\n",
    "        bookBias[book] = beta_I_Sum/(lamb+ len(usersPerBook[book]))\n",
    "    \n",
    "    for user, book, r in training_rating:\n",
    "        model_predictions.append(prediction(user, book))\n",
    "    \n",
    "    if trial == 0:\n",
    "        MSE_0 = 0\n",
    "\n",
    "    MSE_1 = MSE(model_predictions, ratings_train)\n",
    "    \n",
    "    MSE_diff = abs(MSE_1 - MSE_0)\n",
    "\n",
    "    print('MSE_new:', MSE_1)\n",
    "    \n",
    "    MSE_0 = MSE_1\n",
    "    trial += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and MSE on validation data\n",
    "model_predictions = []\n",
    "for user, book, r in validation_rating:\n",
    "    model_predictions.append(prediction(user, book))\n",
    "\n",
    "MSE_valid = MSE(model_predictions, ratings_valid)\n",
    "print('MSE on validation set:',MSE_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User and Book with lowest user bias value\n",
    "minimum = min(userBias, key=userBias.get)  \n",
    "print('User with smallest user bias value:', minimum)\n",
    "print('Smallest user bias value:', userBias[minimum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User and Book with highest user bias value\n",
    "maximum = max(userBias, key=userBias.get)  \n",
    "print('User with largest user bias value:', maximum)\n",
    "print('Largest user bias value:', userBias[maximum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User and Book with lowest book bias value\n",
    "minimum = min(bookBias, key=bookBias.get)  \n",
    "\n",
    "print('Book with smallest book bias value:', minimum)\n",
    "print('Smallest book bias value:', bookBias[minimum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User and Book with highest book bias value\n",
    "maximum = max(bookBias, key=bookBias.get)  \n",
    "\n",
    "print('Book with largest book bias value:', maximum)\n",
    "print('Largest book bias value:', bookBias[maximum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing user and book biases\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "bookRatings = defaultdict(list)\n",
    "\n",
    "for user, book, r in total_set:\n",
    "    r = int(r)\n",
    "    allRatings.append(r)\n",
    "    userRatings[user].append(r)\n",
    "    bookRatings[book].append(r)\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "userAverage = {}\n",
    "userBias = {}\n",
    "user_total = 0\n",
    "bookAverage = {}\n",
    "bookBias = {}\n",
    "book_total = 0\n",
    "\n",
    "# Initializing user and book biases\n",
    "for user in userRatings:\n",
    "    userAverage[user] = sum(userRatings[user]) / len(userRatings[user])\n",
    "    \n",
    "for user in userAverage:\n",
    "    user_total += float(userAverage[user])\n",
    "    \n",
    "for user in userAverage:\n",
    "    userBias[user] = userAverage[user] - (user_total/len(userAverage))\n",
    "\n",
    "for book in bookRatings:\n",
    "    bookAverage[book] = sum(bookRatings[book]) / len(bookRatings[book])\n",
    "    \n",
    "for book in bookAverage:\n",
    "    book_total += float(bookAverage[book])\n",
    "    \n",
    "for book in bookAverage:\n",
    "    bookBias[book] = bookAverage[book] - (book_total/len(bookAverage))\n",
    "    \n",
    "# Defining alpha, bookBias, userBias by convergence from Training data and lambda value 1\n",
    "lambda_value = []\n",
    "MSE_list = []\n",
    "MSE_valid_list = []\n",
    "\n",
    "for lambi in numpy.arange(0, 1, 0.01):\n",
    "    lamb = lambi\n",
    "    alpha_sum = 0\n",
    "    alpha = 0\n",
    "    MSE_diff = 5\n",
    "    trial = 0\n",
    "\n",
    "    \n",
    "    print(lamb)\n",
    "    \n",
    "    while MSE_diff > 0.00005 or trial > 1000:\n",
    "        model_predictions = []\n",
    "        alpha_sum = 0\n",
    "\n",
    "        for user, book, r in training_rating:\n",
    "            alpha_sum += (r-(userBias[user] + bookBias[book]))\n",
    "        alpha = alpha_sum/len(training_rating)\n",
    "\n",
    "        for user in booksPerUser:\n",
    "            beta_U_Sum = 0\n",
    "            for books in booksPerUser[user]:\n",
    "                beta_U_Sum += ratingPerCombo[user,books] - (alpha + bookBias[books])\n",
    "            userBias[user] = beta_U_Sum/(lamb+ len(booksPerUser[user]))\n",
    "\n",
    "        for book in usersPerBook:\n",
    "            beta_I_Sum = 0\n",
    "            for users in usersPerBook[book]:\n",
    "                beta_I_Sum += ratingPerCombo[users,book] - (alpha + userBias[users])\n",
    "            bookBias[book] = beta_I_Sum/(lamb+ len(usersPerBook[book]))\n",
    "\n",
    "        for user, book, r in training_rating:\n",
    "            model_predictions.append(prediction(user, book))\n",
    "\n",
    "        if trial == 0:\n",
    "            MSE_0 = 0\n",
    "\n",
    "        MSE_1 = MSE(model_predictions, ratings_train)\n",
    "\n",
    "        MSE_diff = abs(MSE_1 - MSE_0)\n",
    "\n",
    "        #print('MSE_new:', MSE_1)\n",
    "\n",
    "        MSE_0 = MSE_1\n",
    "        trial += 1 \n",
    "    \n",
    "    lambda_value.append(lambi)\n",
    "    MSE_list.append(MSE_1)\n",
    "    model_predictions = []\n",
    "    for user, book, r in validation_rating:\n",
    "        model_predictions.append(prediction(user, book))\n",
    "\n",
    "    MSE_valid = MSE(model_predictions, ratings_valid)\n",
    "    MSE_valid_list.append(MSE_valid)\n",
    "    print(MSE_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambda_value, MSE_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSE_list.index(min(MSE_list)))\n",
    "print(MSE_list[10])\n",
    "print(lambda_value[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.001\n",
    "alpha_sum = 0\n",
    "alpha = 0\n",
    "MSE_diff = 5\n",
    "trial = 0\n",
    "\n",
    "while MSE_diff > 0.00005 or trial > 1000:\n",
    "    model_predictions = []\n",
    "    alpha_sum = 0\n",
    "\n",
    "    for user, book, r in training_rating:\n",
    "        alpha_sum += (r-(userBias[user] + bookBias[book]))\n",
    "    alpha = alpha_sum/len(training_rating)\n",
    "\n",
    "    for user in booksPerUser:\n",
    "        beta_U_Sum = 0\n",
    "        for books in booksPerUser[user]:\n",
    "            beta_U_Sum += ratingPerCombo[user,books] - (alpha + bookBias[books])\n",
    "        userBias[user] = beta_U_Sum/(lamb+ len(booksPerUser[user]))\n",
    "\n",
    "    for book in usersPerBook:\n",
    "        beta_I_Sum = 0\n",
    "        for users in usersPerBook[book]:\n",
    "            beta_I_Sum += ratingPerCombo[users,book] - (alpha + userBias[users])\n",
    "        bookBias[book] = beta_I_Sum/(lamb+ len(usersPerBook[book]))\n",
    "\n",
    "    for user, book, r in training_rating:\n",
    "        model_predictions.append(prediction(user, book))\n",
    "\n",
    "    if trial == 0:\n",
    "        MSE_0 = 0\n",
    "\n",
    "    MSE_1 = MSE(model_predictions, ratings_train)\n",
    "\n",
    "    MSE_diff = abs(MSE_1 - MSE_0)\n",
    "\n",
    "    print('MSE_new:', MSE_1)\n",
    "\n",
    "    MSE_0 = MSE_1\n",
    "    trial += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and MSE on validation data\n",
    "model_predictions = []\n",
    "for user, book, r in validation_rating:\n",
    "    model_predictions.append(prediction(user, book))\n",
    "\n",
    "MSE_valid = MSE(model_predictions, ratings_valid)\n",
    "print(MSE_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many trials found that lambda value of 3 seems to have the lowest validation MSE at 1.1080896979035464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing predictions of test set to file\n",
    "predictions = open(\"predictions_Rating_Assignment1.txt\", \"w\")\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    \n",
    "    predictions.write(u + \"-\" + b + \",\" + str(prediction(u,b)) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Username: tobycheng or Toby Cheng**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
